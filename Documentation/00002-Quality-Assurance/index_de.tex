\newpage{}

\section{Qualitätssicherung}

\subsection{Allgemein}

Wenn wir in den nächsten Kapiteln Algorithmen betrachten, dann müssen 
wir sagen können, wie gut sie sind. 

Dazu müssen wir einen gemeinsamen Standard etablieren.

Die Ausführung der Algorithmen wird über die `Scripts/configuration.json` 
koordiniert. Hier können Datenquellen angegeben werden, unabhängig davon 
werden die Algorithmen konfiguriert, jeweils mit ihren Aufrufen für "Lernen"
und "Anwenden". 

Des Weiteren kann ein flexibler Parameter angegeben werden, falls für einen Algorithmus
mehrere Varianten existieren. 
Die notwendigen Aufrufe werden dabei automatisch ausmultipliziert.

Datenquellen werden in einer Kombination angegeben. Jeweils eine Datenquelle für 
das Erlernen und eine für den Test. 

\subsection{Aufbau der Eingabedatenquellen}

Wir arbeiten im Rahmen dieser Arbeit ausschließlich mit CSV als Dateiformat. 

- Wir trennen Werte mit dem Komma
- Wir umschließen Werte mit Double-Quotes
- Zahlen werden mit . als Dezimaltrennzeichen angegeben
- Das Umschließen der Zahlen ebenfalls mit Double-Quotes ist erlaubt

- Spalte 1: "Name" (Zeichenkette)
- Spalte 2: "Time spent" (Fließkommazahl)

Die "Time spent" wird in Stunden angegeben. 
In unseren Testdaten ist die Genauigkeit auf 3 Nachkommastellen begrenzt, 
es sollten aber ohne Probleme noch mehr verwendet werde können.

Es können weitere Spalten angegeben werden, falls sie vom Algorithmus verwendet 
werden sollen. In dieser Arbeit verwenden wir aber nur diese beiden. 

\subsection{Aufbau der Ausgabedatenquellen}

Durch die Verarbeitung über einen Algorithmus werden im Verzeichnis Scripts\output
Ausgabe-CSV-Dateien erstellt. Das sind im Grunde die gleichen Dateien mit folgenden
zusätzlichen Spalten:

- Spalte "DurationInSeconds": "Time spent" umgerechnet in Sekunden.
- Spalte "EstimateInSeconds": Die Dauer, die der Algorithmus geschätzt hat, ebenfalls in Sekunden.

\subsection{Tabellarische Analyse}

Am Ende des E-Books wird eine Tabelle mit den Ergebnissen aller Algorithmen über alle 
Datenmengen-Zusammenstellungen automatisch generiert.

Die Tabelle hat folgende Spalten:

- Algorithm: Der Name des Algorithmus
- Training on: Der Name der Trainings-Datenmenge
- Estimating: Der Name der geschätzten Datenmenge
- With Params: Der Parameter, falls es einen gibt, der verwendet wurde
- Deviation: Mittelwert und Standardabweichung des Schätzfehlers in Stunden
- mse: Mean Squared Error, durchschnittlicher quadratischer Fehler

\subsection{Grafische Analyse}

Mit den Werten aus der Tabelle erhält man einen schnellen Überblick. 
Im Grunde könnte man nun behaupten, dass ist es, niedriger Fehler bedeutet alles gut. 
Das leider nicht ganz so einfach.

HIER WEITER

One easy way would be to draw a graph which shows the predictions and
the real values for time spent, while each task is understood as a
category of its own.

We sort that graph by actual duration so we should see the distribution
of durations and around that a hopping range of dots that describes what
the algorithm tells us.

A better algorithm should be closer to the real data. Any algorithm
should never match perfectly, as then we would have a 1:1 mapping. And
that is an over-fit for sure.

\hypertarget{mean-squared-error}{%
\subsection{Mean squared error}\label{mean-squared-error}}

The mean squared error is a common approach to calculate a value for the
quality of an algorithmi. It gets bigger with every estimate we did
wrong.

The formula can be described as:

For every value you predict:

\begin{itemize}
\tightlist
\item
  Calculate the difference between the predicted value and the real
  value
\item
  Sum the squares of each difference
\item
  Divide all the sum by the count of the data entries you check
\end{itemize}

Now, since we want to prevent overfitting we need to prevent
underfitting as well. Since we are talking about seconds and most of the
recorded tasks have a duration in the range of up to 50000 seconds that
means that most tasks are completed in about 13,89 hours. So what about
an error margin of about 5 hours. Which means just as something to think
of, we want the squared error to not exceed squared(5x60x60) =
324.000.000 .

\hypertarget{above-and-below}{%
\subsection{Above and below}\label{above-and-below}}

As a third criteria we have the idea that estimations might even each
other out. In a prefect scenario this would mean that 50\% of the
estimations are too high while the other 50\% are too low. To find out
how good we match we add 1 to a variable for every estimation we find
above the real value and then divide it by the number of tasks. The
result should be .5 when hitting the target.

\hypertarget{the-data}{%
\subsection{The Data}\label{the-data}}

For training and estimating the quality of algorithms I use herein a
dataset that consists of all the tasks that we at the software
development shop at my employer recorded during the time between july
and december 2020 and january 2021 to august 2021. That means there are
two datasets available.

Unfortunately - since this is confidential information - I cannot
publish it alongside this material. But the errors and images can be
shared at it can advance the algorithms - and it is everything that I
have right now. So that will do.

I'll call them: swe2020 and swe2021.

Now let us get a glance at the data as well the first algorithm.
